"""
Photo Classifier - Multithreaded Optimized Version with Fast Pre-check
è‡ªåŠ¨æ ¹æ®åˆ›å»ºæ—¥æœŸåˆ†ç±»å’Œæ•´ç†ç…§ç‰‡è§†é¢‘ï¼Œæ”¯æŒå¤šçº¿ç¨‹å¤„ç†å’Œå¿«é€Ÿé¢„æ£€æŸ¥é¿å…é‡å¤è®¡ç®—hash

Features:
- ğŸš€ Multi-threaded processing for improved performance
- âš¡ Fast pre-check using file size + date to skip duplicate MD5 calculations
- ğŸ”’ Thread-safe database operations with WAL mode
- ğŸ“Š Detailed statistics and progress reporting
- ğŸ›¡ï¸  Comprehensive error handling and logging
"""

import os
import sys
import json
import argparse
import logging
import exifread
import time
import shutil
import hashlib
import sqlite3
import datetime
import pytz
import threading
import queue
from pathlib import Path
from typing import Tuple, Optional, Dict, Any, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from win32com.propsys import propsys, pscon


@dataclass
class FileProcessResult:
    """æ–‡ä»¶å¤„ç†ç»“æœ"""

    md5: str
    file_size: int
    file_type: str
    created_date: str
    original_path: str
    new_path: str
    success: bool
    error_message: Optional[str] = None


class ThreadSafeDatabase:
    """çº¿ç¨‹å®‰å…¨çš„æ•°æ®åº“ç®¡ç†å™¨"""

    def __init__(self, db_path: str, table_name: str):
        self.db_path = db_path
        self.table_name = table_name
        self._local = threading.local()
        self._write_lock = threading.Lock()  # å†™æ“ä½œé”
        self._setup_database()

    def _setup_database(self):
        """è®¾ç½®æ•°æ®åº“ï¼ˆå¯ç”¨WALæ¨¡å¼æå‡å¹¶å‘æ€§èƒ½ï¼‰"""
        try:
            conn = sqlite3.connect(self.db_path)
            # Enable WAL mode for better concurrent access
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=10000")
            conn.execute("PRAGMA temp_store=MEMORY")
            conn.close()
        except sqlite3.Error as e:
            print(f"æ•°æ®åº“è®¾ç½®å¤±è´¥: {e}")

    def get_connection(self):
        """è·å–çº¿ç¨‹æœ¬åœ°æ•°æ®åº“è¿æ¥"""
        if not hasattr(self._local, "connection"):
            self._local.connection = sqlite3.connect(
                self.db_path, check_same_thread=False, timeout=30.0
            )
            self._local.connection.execute("PRAGMA journal_mode=WAL")
        return self._local.connection

    def check_file_exists(self, file_size: int, created_date: str) -> bool:
        """çº¿ç¨‹å®‰å…¨çš„å¿«é€Ÿé¢„æ£€æŸ¥"""
        try:
            conn = self.get_connection()
            cursor = conn.cursor()
            cursor.execute(
                f"SELECT MD5 FROM {self.table_name} WHERE FILE_SIZE=? AND CREATED_DATE=?",
                (file_size, created_date),
            )
            return cursor.fetchone() is not None
        except sqlite3.Error:
            return False

    def check_duplicate(self, md5: str) -> bool:
        """æ£€æŸ¥é‡å¤æ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨è¯»æ“ä½œï¼‰"""
        try:
            conn = self.get_connection()
            cursor = conn.cursor()
            cursor.execute(f"SELECT MD5 FROM {self.table_name} WHERE MD5=?", (md5,))
            return cursor.fetchone() is not None
        except sqlite3.Error:
            return False

    def batch_add_records(self, results: List[FileProcessResult]) -> int:
        """æ‰¹é‡æ·»åŠ è®°å½•ï¼ˆçº¿ç¨‹å®‰å…¨å†™æ“ä½œï¼‰"""
        with self._write_lock:  # ä¸²è¡ŒåŒ–å†™æ“ä½œ
            try:
                conn = self.get_connection()
                cursor = conn.cursor()
                added_count = 0

                conn.execute("BEGIN TRANSACTION")
                for result in results:
                    if result.success:
                        try:
                            cursor.execute(
                                f"INSERT INTO {self.table_name}(MD5, FILE_SIZE, FILE_TYPE, CREATED_DATE) VALUES(?,?,?,?)",
                                (
                                    result.md5,
                                    result.file_size,
                                    result.file_type,
                                    result.created_date,
                                ),
                            )
                            added_count += 1
                        except sqlite3.IntegrityError:
                            continue  # Skip duplicates
                conn.commit()
                return added_count
            except sqlite3.Error as e:
                conn.rollback()
                raise e


class ConfigManager:
    """Configuration management class for loading and validating settings"""

    def __init__(self, config_path: str = "config.json"):
        self.config_path = config_path
        self.config = self._load_config()
        self._validate_config()

    def _load_config(self) -> Dict[str, Any]:
        """Load configuration from JSON file"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"Configuration file not found: {self.config_path}")
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON in configuration file: {e}")

    def _validate_config(self) -> None:
        """Validate required configuration keys"""
        required_keys = ["paths", "supported_formats", "database"]
        for key in required_keys:
            if key not in self.config:
                raise ValueError(f"Missing required configuration key: {key}")

    def get(self, key_path: str, default=None):
        """Get configuration value by dot-separated key path"""
        keys = key_path.split(".")
        value = self.config
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        return value


class PhotoClassifierOptimized:
    """Optimized photo classifier with fast pre-check and enhanced error handling"""

    def __init__(self, config_path: str = "config.json"):
        # Load configuration
        self.config = ConfigManager(config_path)

        # Setup logging
        self._setup_logging()

        # Initialize paths from config
        self.input_folder = self.config.get("paths.input_folder")
        self.photo_output = self.config.get("paths.photo_output")
        self.video_output = self.config.get("paths.video_output")
        self.image_output = self.config.get("paths.image_output")

        # Initialize file extensions
        self.image_extensions = self.config.get(
            "supported_formats.image_extensions", []
        )
        self.video_extensions = self.config.get(
            "supported_formats.video_extensions", []
        )

        # Initialize database settings
        self.table_name = self.config.get("database.table_name", "PHOTO")
        self.db_dir = self.config.get("paths.database_dir", "database")
        self.db_file = self.config.get("paths.database_file", "photo_classifier.db")

        # Initialize EXIF keys
        self.photo_no_date_keys = self.config.get("exif.photo_no_date_keys", [])
        self.photo_date_keys = self.config.get("exif.photo_date_keys", [])
        self.photo_exif_keys = self.photo_no_date_keys + self.photo_date_keys

        # Other settings
        self.skip_folders = self.config.get("skip_folders", [])
        self.timezone = self.config.get("timezone", "Asia/Shanghai")
        self.min_file_size = self.config.get("performance.min_file_size", 1024)

        # Initialize counters (thread-safe)
        self._counter_lock = threading.Lock()
        self.processed_count = 0
        self.error_count = 0
        self.duplicate_count = 0
        self.skipped_count = 0  # å¿«é€Ÿè·³è¿‡çš„æ–‡ä»¶æ•°

        # Multithreading settings
        self.enable_multithreading = self.config.get(
            "performance.enable_multithreading", True
        )
        self.max_workers = self.config.get("performance.max_workers", os.cpu_count())
        self.batch_size = self.config.get("performance.batch_size", 50)

        # Database connections
        self.db = None  # Main database connection
        self.db_path = os.path.join(self.db_dir, self.db_file)
        self.thread_safe_db = None  # Will be initialized when needed

        # Validate configuration
        self._validate_paths()

        mode_desc = "å¤šçº¿ç¨‹" if self.enable_multithreading else "å•çº¿ç¨‹"
        self.logger.info(f"ç…§ç‰‡åˆ†ç±»å™¨åˆå§‹åŒ–æˆåŠŸ - æ”¯æŒå¿«é€Ÿé¢„æ£€æŸ¥ ({mode_desc})")

    def _increment_counter(self, counter_name: str) -> None:
        """çº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨å¢åŠ """
        with self._counter_lock:
            current_value = getattr(self, counter_name)
            setattr(self, counter_name, current_value + 1)

    def _setup_logging(self) -> None:
        """Setup logging configuration"""
        log_level = self.config.get("logging.level", "INFO")
        log_format = self.config.get(
            "logging.format", "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        log_file = self.config.get("logging.file", "photo_classifier.log")

        # Create logger
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(getattr(logging, log_level.upper()))

        # Clear existing handlers
        self.logger.handlers.clear()

        # Create formatter
        formatter = logging.Formatter(log_format)

        # File handler
        file_handler = logging.FileHandler(log_file, encoding="utf-8")
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)

        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)

    def _validate_paths(self) -> None:
        """Validate that required paths exist or can be created"""
        paths_to_check = [
            ("input_folder", self.input_folder, True),  # Must exist
            ("photo_output", self.photo_output, False),  # Can be created
            ("video_output", self.video_output, False),  # Can be created
            ("image_output", self.image_output, False),  # Can be created
        ]

        for path_name, path_value, must_exist in paths_to_check:
            if not path_value:
                raise ValueError(f"Path not configured: {path_name}")

            if must_exist and not os.path.exists(path_value):
                raise FileNotFoundError(f"Required path does not exist: {path_value}")

            if not must_exist:
                try:
                    os.makedirs(path_value, exist_ok=True)
                    self.logger.info(f"åˆ›å»ºè¾“å‡ºç›®å½•: {path_value}")
                except OSError as e:
                    raise OSError(f"Cannot create directory {path_value}: {e}")

    def connect_database(self) -> None:
        """Connect to SQLite database with error handling"""
        try:
            # Ensure database directory exists
            os.makedirs(self.db_dir, exist_ok=True)

            self.db = sqlite3.connect(self.db_path)
            self.db.execute(
                "PRAGMA foreign_keys = ON"
            )  # Enable foreign key constraints
            self.logger.info(f"è¿æ¥åˆ°æ•°æ®åº“: {self.db_path}")
        except sqlite3.Error as e:
            self.logger.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
            raise

    def close_database(self) -> None:
        """Close database connection safely"""
        if self.db:
            try:
                self.db.close()
                self.logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")
            except sqlite3.Error as e:
                self.logger.error(f"å…³é—­æ•°æ®åº“é”™è¯¯: {e}")

    def create_table(self) -> None:
        """Create optimized database table structure with statistics"""
        try:
            self.connect_database()
            cursor = self.db.cursor()

            # Drop existing tables if exists
            cursor.execute(f"DROP TABLE IF EXISTS {self.table_name}")
            cursor.execute("DROP TABLE IF EXISTS STATISTICS")
            self.logger.info(f"åˆ é™¤ç°æœ‰è¡¨: {self.table_name}, STATISTICS")

            # Create new optimized table structure (removed ORIGINAL_PATH and NEW_PATH)
            sql = f"""CREATE TABLE {self.table_name} (
                ID INTEGER PRIMARY KEY AUTOINCREMENT,
                MD5 TEXT NOT NULL UNIQUE,
                FILE_SIZE INTEGER NOT NULL,
                FILE_TYPE TEXT NOT NULL,  -- 'photo', 'image', 'video'
                CREATED_DATE TEXT,
                PROCESSED_DATE TEXT DEFAULT CURRENT_TIMESTAMP
            )"""
            cursor.execute(sql)

            # Create simple statistics table
            stats_sql = """CREATE TABLE STATISTICS (
                ID INTEGER PRIMARY KEY DEFAULT 1,
                PHOTO_COUNT INTEGER DEFAULT 0,     -- ç…§ç‰‡æ•°é‡ï¼ˆæœ‰EXIFï¼‰
                IMAGE_COUNT INTEGER DEFAULT 0,     -- å›¾ç‰‡æ•°é‡ï¼ˆæ— EXIFï¼‰
                VIDEO_COUNT INTEGER DEFAULT 0,     -- è§†é¢‘æ•°é‡
                UPDATED_DATE TEXT DEFAULT CURRENT_TIMESTAMP
            )"""
            cursor.execute(stats_sql)

            # Create indexes for better performance
            cursor.execute(f"CREATE INDEX idx_md5 ON {self.table_name}(MD5)")
            cursor.execute(
                f"CREATE INDEX idx_size_date ON {self.table_name}(FILE_SIZE, CREATED_DATE)"
            )
            cursor.execute(
                f"CREATE INDEX idx_file_type ON {self.table_name}(FILE_TYPE)"
            )

            # Initialize statistics record
            cursor.execute(
                "INSERT INTO STATISTICS (ID, PHOTO_COUNT, IMAGE_COUNT, VIDEO_COUNT) VALUES (1, 0, 0, 0)"
            )

            self.db.commit()
            self.logger.info(
                f"åˆ›å»ºä¼˜åŒ–åçš„è¡¨: {self.table_name}, STATISTICS (æ”¯æŒå¿«é€Ÿé¢„æ£€æŸ¥å’Œåˆ†ç±»ç»Ÿè®¡)"
            )

        except sqlite3.Error as e:
            self.logger.error(f"åˆ›å»ºè¡¨å¤±è´¥: {e}")
            if self.db:
                self.db.rollback()
            raise
        finally:
            self.close_database()

    def _drop_table(self) -> None:
        """Drop database table"""
        try:
            self.connect_database()
            cursor = self.db.cursor()
            cursor.execute(f"DROP TABLE IF EXISTS {self.table_name}")
            cursor.execute("DROP TABLE IF EXISTS STATISTICS")
            self.db.commit()
            self.logger.info(f"åˆ é™¤è¡¨: {self.table_name}, STATISTICS")
        except sqlite3.Error as e:
            self.logger.error(f"åˆ é™¤è¡¨å¤±è´¥: {e}")
            raise
        finally:
            self.close_database()

    def _show_db_info(self) -> None:
        """Show database information"""
        try:
            self.connect_database()
            cursor = self.db.cursor()

            print("=" * 60)
            print("æ•°æ®åº“ä¿¡æ¯")
            print("=" * 60)
            print(f"ğŸ“ æ•°æ®åº“æ–‡ä»¶: {self.db_path}")
            print(f"ğŸ“‹ è¡¨å: {self.table_name}")

            # Check if table exists
            cursor.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
                (self.table_name,),
            )
            if cursor.fetchone():
                # Get record count
                cursor.execute(f"SELECT COUNT(*) FROM {self.table_name}")
                count = cursor.fetchone()[0]
                print(f"ğŸ“Š æ€»è®°å½•æ•°é‡: {count}")

                # Show statistics if available
                cursor.execute(
                    "SELECT name FROM sqlite_master WHERE type='table' AND name='STATISTICS'"
                )
                if cursor.fetchone():
                    cursor.execute(
                        "SELECT PHOTO_COUNT, IMAGE_COUNT, VIDEO_COUNT, UPDATED_DATE FROM STATISTICS WHERE ID=1"
                    )
                    stats = cursor.fetchone()
                    if stats:
                        photo_count, image_count, video_count, updated_date = stats
                        print()
                        print("ğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
                        print(f"   ğŸ“¸ ç…§ç‰‡æ•°é‡ï¼ˆæœ‰EXIFï¼‰: {photo_count}")
                        print(f"   ğŸ–¼ï¸  å›¾ç‰‡æ•°é‡ï¼ˆæ— EXIFï¼‰: {image_count}")
                        print(f"   ğŸ¬ è§†é¢‘æ•°é‡: {video_count}")
                        print(f"   ğŸ•’ æ›´æ–°æ—¶é—´: {updated_date}")

                print()
                # Get table schema
                cursor.execute(f"PRAGMA table_info({self.table_name})")
                columns = cursor.fetchall()
                print("ğŸ—ï¸  è¡¨ç»“æ„:")
                for col in columns:
                    print(f"   {col[1]} ({col[2]})")

                # Show indexes
                cursor.execute(f"PRAGMA index_list({self.table_name})")
                indexes = cursor.fetchall()
                print("ğŸ” ç´¢å¼•:")
                for idx in indexes:
                    print(f"   {idx[1]}")
            else:
                print("âŒ è¡¨ä¸å­˜åœ¨")
            print("=" * 60)

        except sqlite3.Error as e:
            print(f"âŒ æ•°æ®åº“é”™è¯¯: {e}")
        finally:
            self.close_database()

    def _list_records(self, limit: int = 10) -> None:
        """List recent records from database"""
        try:
            self.connect_database()
            cursor = self.db.cursor()

            # Check if table exists
            cursor.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
                (self.table_name,),
            )
            if not cursor.fetchone():
                print("âŒ æ•°æ®åº“è¡¨ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ --create-table")
                return

            # Get recent records (updated for new schema)
            cursor.execute(
                f"SELECT MD5, FILE_SIZE, FILE_TYPE, CREATED_DATE, PROCESSED_DATE FROM {self.table_name} ORDER BY ID DESC LIMIT ?",
                (limit,),
            )
            records = cursor.fetchall()

            print("=" * 80)
            print(f"æœ€è¿‘ {len(records)} æ¡è®°å½•")
            print("=" * 80)

            if records:
                print(
                    f"{'MD5':<20} {'å¤§å°':<8} {'ç±»å‹':<6} {'åˆ›å»ºæ—¥æœŸ':<12} {'å¤„ç†æ—¶é—´'}"
                )
                print("-" * 80)
                for record in records:
                    md5, file_size, file_type, created_date, processed_date = record
                    # Format file size
                    if file_size > 1024 * 1024:
                        size_str = f"{file_size//1024//1024}MB"
                    elif file_size > 1024:
                        size_str = f"{file_size//1024}KB"
                    else:
                        size_str = f"{file_size}B"

                    # Format file type display
                    type_display = {
                        "photo": "ğŸ“¸ç…§ç‰‡",
                        "image": "ğŸ–¼ï¸å›¾ç‰‡",
                        "video": "ğŸ¬è§†é¢‘",
                    }.get(file_type, file_type)

                    print(
                        f"{md5[:8]}...{md5[-8:]} {size_str:<8} {type_display:<6} {created_date:<12} {processed_date[:19]}"
                    )
            else:
                print("ğŸ“­ æ•°æ®åº“ä¸ºç©º")
            print("=" * 80)

        except sqlite3.Error as e:
            print(f"âŒ æ•°æ®åº“é”™è¯¯: {e}")
        finally:
            self.close_database()

    def is_valid_file_size(self, file_path: str) -> bool:
        """Check if file size meets minimum requirements"""
        try:
            file_size = os.path.getsize(file_path)
            return file_size >= self.min_file_size
        except OSError:
            return False

    def is_photo(self, file_path: str) -> bool:
        """Check if file is a photo with EXIF data"""
        return self.is_image(file_path) and self.contains_exif(file_path)

    def is_video(self, file_path: str) -> bool:
        """Check if file is a video"""
        return any(file_path.lower().endswith(ext) for ext in self.video_extensions)

    def is_image(self, file_path: str) -> bool:
        """Check if file is an image"""
        return any(file_path.lower().endswith(ext) for ext in self.image_extensions)

    def contains_exif(self, file_path: str) -> bool:
        """Check if image contains EXIF data"""
        try:
            with open(file_path, "rb") as reader:
                tags = exifread.process_file(reader)
                return any(key in tags for key in self.photo_exif_keys)
        except (IOError, OSError) as e:
            self.logger.warning(f"æ— æ³•ä» {file_path} è¯»å–EXIF: {e}")
            return False

    def should_process_file(self, file_path: str) -> bool:
        """ğŸš€ Fast pre-check to determine if file needs processing"""
        try:
            # Get file size
            file_size = os.path.getsize(file_path)

            # Extract creation date (this is the main cost, but still faster than MD5)
            year, month, day = self.read_date(file_path)
            created_date = f"{year}-{month}-{day}"

            # Check if file already exists with same size and creation date
            cursor = self.db.cursor()
            cursor.execute(
                f"SELECT MD5 FROM {self.table_name} WHERE FILE_SIZE=? AND CREATED_DATE=?",
                (file_size, created_date),
            )

            if cursor.fetchone():
                self.logger.debug(f"æ–‡ä»¶å·²å­˜åœ¨ï¼ˆå¤§å°+æ—¥æœŸåŒ¹é…ï¼‰ï¼Œè·³è¿‡: {file_path}")
                self.skipped_count += 1
                return False

            return True

        except (OSError, sqlite3.Error) as e:
            self.logger.warning(f"é¢„æ£€æŸ¥ {file_path} å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶ä»ç„¶å¤„ç†ï¼Œè®©åç»­æµç¨‹å¤„ç†

    def get_md5(self, file_path: str) -> str:
        """Calculate MD5 hash of file with error handling"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except (IOError, OSError) as e:
            self.logger.error(f"è®¡ç®— {file_path} çš„MD5å¤±è´¥: {e}")
            raise

    def validate_file(self, file_path: str, md5: str) -> None:
        """Validate file and check for duplicates"""
        try:
            cursor = self.db.cursor()
            cursor.execute(f"SELECT MD5 FROM {self.table_name} WHERE MD5=?", (md5,))
            record = cursor.fetchone()

            if record:
                self.logger.warning(f"å‘ç°é‡å¤æ–‡ä»¶ï¼ˆMD5ç›¸åŒï¼‰: {file_path}")
                os.remove(file_path)
                self.duplicate_count += 1
                raise ValueError(f"Duplicate file removed: {file_path}")

        except sqlite3.Error as e:
            self.logger.error(f"éªŒè¯æœŸé—´æ•°æ®åº“é”™è¯¯: {e}")
            raise
        except OSError as e:
            self.logger.error(f"åˆ é™¤é‡å¤æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            raise

    def get_photo_create_date(self, file_path: str) -> Optional[Tuple[str, str, str]]:
        """Extract creation date from photo EXIF data"""
        try:
            with open(file_path, "rb") as reader:
                tags = exifread.process_file(reader)
                for key in self.photo_date_keys:
                    if key in tags:
                        time_str = str(tags[key])
                        if ":" in time_str[:10]:
                            date_parts = time_str[:10].split(":")
                            if len(date_parts) == 3:
                                return tuple(date_parts)
            return None
        except (IOError, OSError) as e:
            self.logger.warning(f"æ— æ³•ä» {file_path} è¯»å–ç…§ç‰‡æ—¥æœŸ: {e}")
            return None

    def get_video_create_date(self, file_path: str) -> Optional[Tuple[str, str, str]]:
        """Extract creation date from video metadata"""
        try:
            properties = propsys.SHGetPropertyStoreFromParsingName(file_path)
            dt = properties.GetValue(pscon.PKEY_Media_DateEncoded).GetValue()
            if dt:
                time_str = str(dt.astimezone(pytz.timezone(self.timezone)))
                date_parts = time_str[:10].split("-")
                if len(date_parts) == 3:
                    return tuple(date_parts)
            return None
        except Exception as e:
            self.logger.warning(f"æ— æ³•ä» {file_path} è¯»å–è§†é¢‘æ—¥æœŸ: {e}")
            return None

    def read_date(self, file_path: str) -> Tuple[str, str, str]:
        """Read creation date from file with fallback to modification time"""
        file_path = file_path.replace("/", "\\")
        date = None

        if self.is_photo(file_path):
            date = self.get_photo_create_date(file_path)
        elif self.is_video(file_path):
            date = self.get_video_create_date(file_path)

        if not date:
            # Fallback to file modification time
            try:
                mtime = os.path.getmtime(file_path)
                dt = datetime.datetime.fromtimestamp(mtime)
                date = (str(dt.year), f"{dt.month:02d}", f"{dt.day:02d}")
                self.logger.info(f"ä½¿ç”¨æ–‡ä»¶ {file_path} çš„ä¿®æ”¹æ—¥æœŸ")
            except OSError as e:
                self.logger.error(f"æ— æ³•è·å–æ–‡ä»¶ {file_path} çš„ä¿®æ”¹æ—¶é—´: {e}")
                # Use current date as last resort
                now = datetime.datetime.now()
                date = (str(now.year), f"{now.month:02d}", f"{now.day:02d}")

        return date

    def rename_move(
        self, file_path: str, year: str, month: str, day: str, md5: str
    ) -> str:
        """Rename and move file to appropriate directory"""
        # Determine output directory
        if self.is_image(file_path):
            output_dir = (
                self.photo_output if self.is_photo(file_path) else self.image_output
            )
        elif self.is_video(file_path):
            output_dir = self.video_output
        else:
            raise ValueError(f"Unsupported file type: {file_path}")

        # Create target directory
        target_dir = os.path.join(output_dir, year, month, day)
        os.makedirs(target_dir, exist_ok=True)

        # Generate new filename
        _, file_ext = os.path.splitext(file_path)
        new_name = f"{year}-{month}-{day}-{md5}{file_ext}"
        target_path = os.path.join(target_dir, new_name)

        # Handle filename conflicts
        counter = 1
        while os.path.exists(target_path):
            base_name = f"{year}-{month}-{day}-{md5}_{counter}{file_ext}"
            target_path = os.path.join(target_dir, base_name)
            counter += 1

        # Move file
        try:
            shutil.move(file_path, target_path)
            self.logger.info(f"ç§»åŠ¨: {file_path} -> {target_path}")
            return os.path.basename(target_path)
        except (IOError, OSError) as e:
            self.logger.error(f"ç§»åŠ¨æ–‡ä»¶ {file_path} åˆ° {target_path} å¤±è´¥: {e}")
            raise

    def add_record(
        self, md5: str, file_size: int, file_type: str, created_date: str
    ) -> None:
        """Add file record to database with optimized metadata (removed path fields)"""
        try:
            cursor = self.db.cursor()
            cursor.execute(
                f"INSERT INTO {self.table_name}(MD5, FILE_SIZE, FILE_TYPE, CREATED_DATE) VALUES(?,?,?,?)",
                (md5, file_size, file_type, created_date),
            )
            self.db.commit()
        except sqlite3.Error as e:
            self.logger.error(f"æ·»åŠ è®°å½• {md5} å¤±è´¥: {e}")
            self.db.rollback()
            raise

    def process_file_single(self, file_path: str) -> FileProcessResult:
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå¤šçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰"""
        try:
            # Check file size
            if not self.is_valid_file_size(file_path):
                self.logger.debug(f"æ–‡ä»¶å¤ªå°ï¼Œè·³è¿‡: {file_path}")
                return None

            # Check if it's a supported file type
            if not (self.is_image(file_path) or self.is_video(file_path)):
                self.logger.debug(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œè·³è¿‡: {file_path}")
                return None

            # Get file metadata for fast pre-check
            file_size = os.path.getsize(file_path)
            year, month, day = self.read_date(file_path)
            created_date = f"{year}-{month}-{day}"

            # ğŸš€ Fast pre-check using thread-safe database
            if self.thread_safe_db.check_file_exists(file_size, created_date):
                self.logger.debug(f"æ–‡ä»¶å·²å­˜åœ¨ï¼ˆå¤§å°+æ—¥æœŸåŒ¹é…ï¼‰ï¼Œè·³è¿‡: {file_path}")
                self._increment_counter("skipped_count")
                return None

            # Calculate MD5 (this is the expensive operation)
            self.logger.debug(f"å¼€å§‹è®¡ç®—MD5: {file_path}")
            md5 = self.get_md5(file_path)

            # Check for MD5 duplicates
            if self.thread_safe_db.check_duplicate(md5):
                self.logger.warning(f"å‘ç°é‡å¤æ–‡ä»¶ï¼ˆMD5ç›¸åŒï¼‰: {file_path}")
                try:
                    os.remove(file_path)
                    self._increment_counter("duplicate_count")
                except OSError as e:
                    self.logger.error(f"åˆ é™¤é‡å¤æ–‡ä»¶å¤±è´¥: {e}")
                return None

            # Determine file type
            if self.is_photo(file_path):
                file_type = "photo"
            elif self.is_video(file_path):
                file_type = "video"
            else:
                file_type = "image"  # Image without EXIF

            # Move and rename file
            new_name = self.rename_move(file_path, year, month, day, md5)
            new_path = os.path.join(
                (
                    self.photo_output
                    if file_type == "photo"
                    else (
                        self.video_output if file_type == "video" else self.image_output
                    )
                ),
                year,
                month,
                day,
                new_name,
            )

            self._increment_counter("processed_count")
            self.logger.info(
                f"å·²å¤„ç† ({self.processed_count}): {os.path.basename(file_path)} -> {new_name}"
            )

            return FileProcessResult(
                md5=md5,
                file_size=file_size,
                file_type=file_type,
                created_date=created_date,
                original_path=file_path,
                new_path=new_path,
                success=True,
            )

        except Exception as e:
            self._increment_counter("error_count")
            self.logger.error(f"å¤„ç† {file_path} é”™è¯¯: {e}")
            return FileProcessResult(
                md5="",
                file_size=0,
                file_type="",
                created_date="",
                original_path=file_path,
                new_path="",
                success=False,
                error_message=str(e),
            )

    def process_file(self, root: str, filename: str) -> None:
        """Process a single file with fast pre-check and comprehensive error handling (legacy method)"""
        file_path = os.path.join(root, filename)

        try:
            # Check file size
            if not self.is_valid_file_size(file_path):
                self.logger.debug(f"æ–‡ä»¶å¤ªå°ï¼Œè·³è¿‡: {file_path}")
                return

            # Check if it's a supported file type
            if not (self.is_image(file_path) or self.is_video(file_path)):
                self.logger.debug(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œè·³è¿‡: {file_path}")
                return

            # ğŸš€ Fast pre-check: Skip if file already processed and unchanged
            if not self.should_process_file(file_path):
                return

            # Calculate MD5 only if needed
            self.logger.debug(f"å¼€å§‹è®¡ç®—MD5: {file_path}")
            md5 = self.get_md5(file_path)
            file_size = os.path.getsize(file_path)

            # Validate (check for duplicates)
            self.validate_file(file_path, md5)

            # Determine file type
            if self.is_photo(file_path):
                file_type = "photo"
            elif self.is_video(file_path):
                file_type = "video"
            else:
                file_type = "image"  # Image without EXIF

            # Extract date
            year, month, day = self.read_date(file_path)
            created_date = f"{year}-{month}-{day}"

            # Move and rename file
            new_name = self.rename_move(file_path, year, month, day, md5)

            # Add record to database (without path information)
            self.add_record(md5, file_size, file_type, created_date)

            self.processed_count += 1
            self.logger.info(
                f"å·²å¤„ç† ({self.processed_count}): {filename} -> {new_name}"
            )

        except Exception as e:
            self.error_count += 1
            self.logger.error(f"å¤„ç† {file_path} é”™è¯¯: {e}")

    def collect_files(self, folder: str) -> List[str]:
        """æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶"""
        file_paths = []
        self.logger.info(f"æ”¶é›†æ–‡ä»¶: {folder}")

        for root, dirs, files in os.walk(folder):
            # Skip system folders
            dirs[:] = [d for d in dirs if d not in self.skip_folders]

            for filename in files:
                file_path = os.path.join(root, filename)
                if (
                    self.is_image(file_path) or self.is_video(file_path)
                ) and self.is_valid_file_size(file_path):
                    file_paths.append(file_path)

        self.logger.info(f"å‘ç° {len(file_paths)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
        return file_paths

    def process_folder_multithreaded(self, folder: str) -> None:
        """å¤šçº¿ç¨‹å¤„ç†æ–‡ä»¶å¤¹"""
        self.logger.info(
            f"å¼€å§‹å¤šçº¿ç¨‹å¤„ç†æ–‡ä»¶å¤¹: {folder} (å·¥ä½œçº¿ç¨‹: {self.max_workers})"
        )

        # Initialize thread-safe database
        self.thread_safe_db = ThreadSafeDatabase(self.db_path, self.table_name)

        # Collect all files first
        file_paths = self.collect_files(folder)
        if not file_paths:
            self.logger.info("æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡ä»¶")
            return

        # Process files in batches using thread pool
        total_files = len(file_paths)
        processed_batches = 0

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Process files in batches
            for i in range(0, total_files, self.batch_size):
                batch = file_paths[i : i + self.batch_size]
                batch_results = []

                # Submit batch to thread pool
                future_to_file = {
                    executor.submit(self.process_file_single, file_path): file_path
                    for file_path in batch
                }

                # Collect results
                for future in as_completed(future_to_file):
                    result = future.result()
                    if result is not None:
                        batch_results.append(result)

                # Batch write to database
                if batch_results:
                    try:
                        added_count = self.thread_safe_db.batch_add_records(
                            batch_results
                        )
                        processed_batches += 1
                        self.logger.info(
                            f"æ‰¹æ¬¡ {processed_batches} å®Œæˆï¼Œæ·»åŠ äº† {added_count} æ¡è®°å½•"
                        )
                    except Exception as e:
                        self.logger.error(f"æ‰¹é‡å†™å…¥æ•°æ®åº“å¤±è´¥: {e}")

        self.logger.info(f"å¤šçº¿ç¨‹å¤„ç†å®Œæˆï¼Œæ€»å…±å¤„ç†äº† {processed_batches} ä¸ªæ‰¹æ¬¡")

    def process_folder(self, folder: str) -> None:
        """Process all files in folder recursively"""
        if self.enable_multithreading:
            self.process_folder_multithreaded(folder)
        else:
            self.logger.info(f"å¼€å§‹å•çº¿ç¨‹å¤„ç†æ–‡ä»¶å¤¹: {folder}")
            for root, dirs, files in os.walk(folder):
                # Skip system folders
                dirs[:] = [d for d in dirs if d not in self.skip_folders]

                for filename in files:
                    self.process_file(root, filename)

    def delete_empty_folders(self, folder: str) -> None:
        """Delete empty folders after processing"""
        deleted_count = 0
        for root, dirs, files in os.walk(folder, topdown=False):
            for dir_name in dirs:
                if dir_name in self.skip_folders:
                    continue

                dir_path = os.path.join(root, dir_name)
                try:
                    if not os.listdir(dir_path):  # Check if directory is empty
                        os.rmdir(dir_path)
                        deleted_count += 1
                        self.logger.info(f"åˆ é™¤ç©ºç›®å½•: {dir_path}")
                except OSError as e:
                    self.logger.warning(f"æ— æ³•åˆ é™¤ç›®å½• {dir_path}: {e}")

        self.logger.info(f"åˆ é™¤äº† {deleted_count} ä¸ªç©ºç›®å½•")

    def update_statistics(self) -> None:
        """Update statistics table with current data"""
        try:
            cursor = self.db.cursor()

            # Count files by type
            cursor.execute(
                f"SELECT COUNT(*) FROM {self.table_name} WHERE FILE_TYPE='photo'"
            )
            photo_count = cursor.fetchone()[0]

            cursor.execute(
                f"SELECT COUNT(*) FROM {self.table_name} WHERE FILE_TYPE='image'"
            )
            image_count = cursor.fetchone()[0]

            cursor.execute(
                f"SELECT COUNT(*) FROM {self.table_name} WHERE FILE_TYPE='video'"
            )
            video_count = cursor.fetchone()[0]

            # Update statistics table
            cursor.execute(
                """
                UPDATE STATISTICS SET 
                    PHOTO_COUNT = ?, 
                    IMAGE_COUNT = ?, 
                    VIDEO_COUNT = ?, 
                    UPDATED_DATE = CURRENT_TIMESTAMP 
                WHERE ID = 1
            """,
                (photo_count, image_count, video_count),
            )

            self.db.commit()
            self.logger.info(
                f"æ›´æ–°ç»Ÿè®¡æ•°æ®: ç…§ç‰‡{photo_count}å¼ , å›¾ç‰‡{image_count}å¼ , è§†é¢‘{video_count}ä¸ª"
            )

        except sqlite3.Error as e:
            self.logger.error(f"æ›´æ–°ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")

    def generate_report(self) -> None:
        """Generate processing report with optimization and multithreading statistics"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š å¤„ç†æŠ¥å‘Š")
        self.logger.info("=" * 60)
        mode = "å¤šçº¿ç¨‹" if self.enable_multithreading else "å•çº¿ç¨‹"
        if self.enable_multithreading:
            self.logger.info(
                f"ğŸ”§ å¤„ç†æ¨¡å¼: {mode} (å·¥ä½œçº¿ç¨‹: {self.max_workers}, æ‰¹é‡å¤§å°: {self.batch_size})"
            )
        else:
            self.logger.info(f"ğŸ”§ å¤„ç†æ¨¡å¼: {mode}")
        self.logger.info(f"âœ… å·²å¤„ç†æ–‡ä»¶: {self.processed_count}")
        self.logger.info(f"âš¡ å¿«é€Ÿè·³è¿‡: {self.skipped_count}")
        self.logger.info(f"ğŸ”„ å‘ç°é‡å¤: {self.duplicate_count}")
        self.logger.info(f"âŒ é‡åˆ°é”™è¯¯: {self.error_count}")
        total_checked = (
            self.processed_count
            + self.skipped_count
            + self.duplicate_count
            + self.error_count
        )
        if total_checked > 0:
            skip_ratio = (self.skipped_count / total_checked) * 100
            self.logger.info(f"ğŸš€ é¢„æ£€æŸ¥ä¼˜åŒ–ç‡: {skip_ratio:.1f}%")
        self.logger.info("=" * 60)

    def start(self) -> None:
        """Start the classification process"""
        start_time = time.time()
        self.logger.info("å¼€å§‹ç…§ç‰‡åˆ†ç±»å¤„ç† - æ”¯æŒå¿«é€Ÿé¢„æ£€æŸ¥")

        try:
            self.connect_database()
            self.process_folder(self.input_folder)
            self.delete_empty_folders(self.input_folder)

            # Update statistics after processing
            self.update_statistics()

        except Exception as e:
            self.logger.error(f"å¤„ç†æœŸé—´è‡´å‘½é”™è¯¯: {e}")
            raise
        finally:
            self.close_database()

        end_time = time.time()
        duration = end_time - start_time
        self.logger.info(f"å¤„ç†å®Œæˆï¼Œç”¨æ—¶ {duration:.2f} ç§’")
        self.generate_report()


def main():
    """Main function with command line argument support"""
    parser = argparse.ArgumentParser(
        description="Photo Classifier - Multithreaded Fast Pre-check Optimized Version"
    )
    parser.add_argument(
        "--config", default="config.json", help="Configuration file path"
    )
    parser.add_argument(
        "--create-table", action="store_true", help="Create/recreate database table"
    )
    parser.add_argument(
        "--drop-table", action="store_true", help="Drop existing database table"
    )
    parser.add_argument(
        "--list-records", action="store_true", help="List all records in database"
    )
    parser.add_argument(
        "--db-info", action="store_true", help="Show database information"
    )
    parser.add_argument(
        "--stats", action="store_true", help="Update and show statistics"
    )
    parser.add_argument("--input", help="Override input folder from config")
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Enable verbose logging"
    )
    parser.add_argument(
        "--single-thread", action="store_true", help="Force single-threaded mode"
    )
    parser.add_argument(
        "--max-workers", type=int, help="Maximum number of worker threads"
    )
    parser.add_argument(
        "--batch-size", type=int, help="Batch size for processing files"
    )

    args = parser.parse_args()

    try:
        # Initialize classifier
        classifier = PhotoClassifierOptimized(args.config)

        # Override input folder if provided
        if args.input:
            classifier.input_folder = args.input
            classifier.logger.info(f"è¾“å…¥æ–‡ä»¶å¤¹å·²è¦†ç›–ä¸º: {args.input}")

        # Set multithreading options
        if args.single_thread:
            classifier.enable_multithreading = False
            classifier.logger.info("å¼ºåˆ¶ä½¿ç”¨å•çº¿ç¨‹æ¨¡å¼")

        if args.max_workers:
            classifier.max_workers = args.max_workers
            classifier.logger.info(f"æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°è®¾ç½®ä¸º: {args.max_workers}")

        if args.batch_size:
            classifier.batch_size = args.batch_size
            classifier.logger.info(f"æ‰¹é‡å¤„ç†å¤§å°è®¾ç½®ä¸º: {args.batch_size}")

        # Set verbose logging if requested
        if args.verbose:
            classifier.logger.setLevel(logging.DEBUG)

        # Handle database operations
        if args.create_table:
            print("æ­£åœ¨åˆ›å»º/é‡å»ºæ•°æ®åº“è¡¨...")
            classifier.create_table()
            print("âœ… æ•°æ®åº“è¡¨åˆ›å»ºå®Œæˆï¼ˆæ”¯æŒå¿«é€Ÿé¢„æ£€æŸ¥å’Œç»Ÿè®¡ï¼‰")
            print(f"ğŸ“ æ•°æ®åº“ä½ç½®: {classifier.db_path}")
            print(f"ğŸ“‹ è¡¨å: {classifier.table_name}")
            return

        if args.drop_table:
            print("æ­£åœ¨åˆ é™¤æ•°æ®åº“è¡¨...")
            classifier._drop_table()
            print("âœ… æ•°æ®åº“è¡¨åˆ é™¤å®Œæˆ")
            return

        if args.db_info:
            classifier._show_db_info()
            return

        if args.list_records:
            classifier._list_records()
            return

        if args.stats:
            print("æ­£åœ¨æ›´æ–°ç»Ÿè®¡æ•°æ®...")
            classifier.connect_database()
            classifier.update_statistics()
            classifier.close_database()
            print("âœ… ç»Ÿè®¡æ•°æ®æ›´æ–°å®Œæˆ")
            classifier._show_db_info()
            return

        # Start processing
        classifier.start()

    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    # ğŸ¯ IDEå¼€å‘æ¨¡å¼ï¼šç›´æ¥åœ¨ä»£ç ä¸­è®¾ç½®å‚æ•°ï¼Œæ— éœ€å‘½ä»¤è¡Œè¾“å…¥
    import sys

    # ğŸ’¡ å–æ¶ˆæ³¨é‡Šä½ éœ€è¦çš„åŠŸèƒ½ï¼ˆåªèƒ½åŒæ—¶å¯ç”¨ä¸€ä¸ªï¼‰ï¼š

    # === æ•°æ®åº“æ“ä½œ ===
    # sys.argv = ["script_name", "--create-table"]  # åˆ›å»ºæ•°æ®åº“è¡¨
    # sys.argv = ["script_name", "--db-info"]                   # æŸ¥çœ‹æ•°æ®åº“ä¿¡æ¯
    # sys.argv = ["script_name", "--list-records"]              # æŸ¥çœ‹æœ€è¿‘è®°å½•
    # sys.argv = ["script_name", "--stats"]                     # æ›´æ–°å¹¶æ˜¾ç¤ºç»Ÿè®¡æ•°æ®
    # sys.argv = ["script_name", "--drop-table"]                # åˆ é™¤æ•°æ®åº“è¡¨

    # === å¤„ç†æ¨¡å¼ ===
    # sys.argv = ["script_name", "--verbose"]                   # å¯ç”¨è¯¦ç»†æ—¥å¿—
    # sys.argv = ["script_name", "--single-thread"]             # å¼ºåˆ¶å•çº¿ç¨‹æ¨¡å¼
    # sys.argv = ["script_name", "--max-workers", "8"]          # è®¾ç½®æœ€å¤§çº¿ç¨‹æ•°
    # sys.argv = ["script_name", "--batch-size", "100"]         # è®¾ç½®æ‰¹å¤„ç†å¤§å°

    # === è‡ªå®šä¹‰é…ç½® ===
    # sys.argv = ["script_name", "--input", "D:\\test\\input"]  # è‡ªå®šä¹‰è¾“å…¥ç›®å½•
    # sys.argv = ["script_name", "--config", "my_config.json"]  # è‡ªå®šä¹‰é…ç½®æ–‡ä»¶

    # === ç»„åˆä½¿ç”¨ç¤ºä¾‹ ===
    # sys.argv = ["script_name", "--verbose", "--max-workers", "6", "--batch-size", "50"]  # å¤šçº¿ç¨‹è¯¦ç»†æ¨¡å¼

    # é»˜è®¤è¿è¡Œï¼šå¤šçº¿ç¨‹ç…§ç‰‡åˆ†ç±»å¤„ç†ï¼ˆæ³¨é‡Šæ‰ä¸Šé¢æ‰€æœ‰é€‰é¡¹æ—¶ä½¿ç”¨ï¼‰
    # sys.argv = ["script_name"]

    main()
